{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "dJg7pIZBG8_7",
        "Gf2E9OkhThEs"
      ],
      "mount_file_id": "169MyiCyS-dt3k2ES98BTM8dtJyTm4Yt0",
      "authorship_tag": "ABX9TyNT2JVjcAIgjy5Wx9QnTxcA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ftmthb/NER/blob/main/NER_fine_tune_classes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "T5MCxu348uVa"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/drive/MyDrive/virtual_env/lib/python3.10/site-packages\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81qG4gRiGOwj"
      },
      "source": [
        "### Data import\n",
        "+only taking dcts with annotations, clean,"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ldMlBFcJ2YuE"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def import_func(path):\n",
        "  ext = str(path).split('.')[-1]\n",
        "  if ext == 'json':\n",
        "    import json\n",
        "    with open(path) as file:\n",
        "      raw_data = json.load(file)\n",
        "  elif ext == 'pkl':\n",
        "    import pickle\n",
        "    with open(path, 'rb') as file:\n",
        "      raw_data = pickle.load(file)\n",
        "  else:\n",
        "    raise ValueError(\"Unsupported file format. Only JSON (.json) and Pickle (.pkl) files are supported.\")\n",
        "  return raw_data\n",
        "train_data = import_func(r\"/content/drive/MyDrive/ner_india/NER_TRAIN_JUDGEMENT.json\")\n",
        "dev_data = import_func(r\"/content/drive/MyDrive/ner_india/NER_DEV_PREAMBLE.json\")\n",
        "# data[0].keys()"
      ],
      "metadata": {
        "id": "TUd4yOuCGPUV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### draft"
      ],
      "metadata": {
        "id": "dJg7pIZBG8_7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0nd9QuBfBvG",
        "outputId": "7b524090-1df9-4465-e49a-e5ef8f23eed7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9435"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "with open(r\"/content/drive/MyDrive/ner_india/NER_TRAIN_JUDGEMENT.json\") as file:\n",
        "    json_data = json.load(file)\n",
        "len(json_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wH5EONDdoL1e",
        "outputId": "cc0ed85d-1623-49b5-8cd8-d714ad000d54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "125\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "with open(r\"/content/drive/MyDrive/ner_india/NER_DEV_PREAMBLE.json\") as file:\n",
        "    dev_data = json.load(file)\n",
        "print(len(dev_data))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[3]['annotations'] == [{'result': []}]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89_xOpGZ-BhP",
        "outputId": "5ca085cc-aa81-4c24-dac3-750cf690a687"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for indice in range(len(json_data)):\n",
        "  for annot in json_data[indice]['annotations']:\n",
        "    if annot['result']== []:\n",
        "      print(indice)\n",
        "      break #3, 6, 8, 16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mDykcDSw9zjb",
        "outputId": "b804a1c0-13f7-4dfd-eba2-737072b8705d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "6\n",
            "8\n",
            "16\n",
            "19\n",
            "22\n",
            "23\n",
            "29\n",
            "32\n",
            "38\n",
            "40\n",
            "42\n",
            "45\n",
            "48\n",
            "66\n",
            "67\n",
            "69\n",
            "76\n",
            "80\n",
            "81\n",
            "87\n",
            "89\n",
            "90\n",
            "99\n",
            "108\n",
            "109\n",
            "110\n",
            "114\n",
            "118\n",
            "119\n",
            "122\n",
            "125\n",
            "127\n",
            "128\n",
            "129\n",
            "132\n",
            "133\n",
            "134\n",
            "137\n",
            "146\n",
            "147\n",
            "150\n",
            "152\n",
            "154\n",
            "161\n",
            "164\n",
            "169\n",
            "173\n",
            "178\n",
            "183\n",
            "190\n",
            "192\n",
            "194\n",
            "196\n",
            "199\n",
            "200\n",
            "203\n",
            "204\n",
            "210\n",
            "211\n",
            "218\n",
            "220\n",
            "225\n",
            "228\n",
            "234\n",
            "235\n",
            "241\n",
            "255\n",
            "256\n",
            "259\n",
            "260\n",
            "261\n",
            "265\n",
            "266\n",
            "271\n",
            "278\n",
            "280\n",
            "288\n",
            "292\n",
            "293\n",
            "295\n",
            "296\n",
            "305\n",
            "306\n",
            "316\n",
            "317\n",
            "324\n",
            "326\n",
            "350\n",
            "361\n",
            "364\n",
            "370\n",
            "373\n",
            "381\n",
            "382\n",
            "387\n",
            "392\n",
            "396\n",
            "397\n",
            "405\n",
            "406\n",
            "411\n",
            "414\n",
            "418\n",
            "423\n",
            "425\n",
            "431\n",
            "432\n",
            "441\n",
            "442\n",
            "444\n",
            "450\n",
            "459\n",
            "462\n",
            "464\n",
            "465\n",
            "468\n",
            "480\n",
            "487\n",
            "489\n",
            "500\n",
            "507\n",
            "508\n",
            "530\n",
            "543\n",
            "545\n",
            "546\n",
            "547\n",
            "549\n",
            "555\n",
            "561\n",
            "567\n",
            "572\n",
            "579\n",
            "585\n",
            "621\n",
            "624\n",
            "626\n",
            "627\n",
            "630\n",
            "636\n",
            "637\n",
            "639\n",
            "644\n",
            "645\n",
            "646\n",
            "647\n",
            "650\n",
            "654\n",
            "659\n",
            "661\n",
            "664\n",
            "675\n",
            "676\n",
            "679\n",
            "688\n",
            "702\n",
            "722\n",
            "727\n",
            "731\n",
            "732\n",
            "743\n",
            "745\n",
            "746\n",
            "750\n",
            "758\n",
            "761\n",
            "768\n",
            "771\n",
            "772\n",
            "778\n",
            "780\n",
            "784\n",
            "787\n",
            "793\n",
            "801\n",
            "804\n",
            "805\n",
            "806\n",
            "809\n",
            "814\n",
            "825\n",
            "829\n",
            "838\n",
            "839\n",
            "849\n",
            "859\n",
            "871\n",
            "879\n",
            "880\n",
            "883\n",
            "889\n",
            "890\n",
            "895\n",
            "901\n",
            "903\n",
            "906\n",
            "907\n",
            "916\n",
            "919\n",
            "920\n",
            "921\n",
            "922\n",
            "927\n",
            "929\n",
            "934\n",
            "937\n",
            "941\n",
            "944\n",
            "955\n",
            "960\n",
            "962\n",
            "974\n",
            "977\n",
            "980\n",
            "990\n",
            "991\n",
            "992\n",
            "998\n",
            "1000\n",
            "1011\n",
            "1013\n",
            "1016\n",
            "1019\n",
            "1022\n",
            "1023\n",
            "1036\n",
            "1039\n",
            "1055\n",
            "1063\n",
            "1069\n",
            "1074\n",
            "1077\n",
            "1088\n",
            "1089\n",
            "1094\n",
            "1103\n",
            "1109\n",
            "1113\n",
            "1117\n",
            "1118\n",
            "1122\n",
            "1137\n",
            "1141\n",
            "1143\n",
            "1144\n",
            "1147\n",
            "1148\n",
            "1153\n",
            "1155\n",
            "1166\n",
            "1168\n",
            "1172\n",
            "1174\n",
            "1182\n",
            "1191\n",
            "1194\n",
            "1196\n",
            "1198\n",
            "1199\n",
            "1208\n",
            "1212\n",
            "1213\n",
            "1214\n",
            "1225\n",
            "1231\n",
            "1234\n",
            "1235\n",
            "1237\n",
            "1238\n",
            "1242\n",
            "1245\n",
            "1256\n",
            "1257\n",
            "1263\n",
            "1270\n",
            "1272\n",
            "1273\n",
            "1278\n",
            "1279\n",
            "1280\n",
            "1281\n",
            "1282\n",
            "1284\n",
            "1287\n",
            "1291\n",
            "1296\n",
            "1298\n",
            "1303\n",
            "1306\n",
            "1309\n",
            "1313\n",
            "1314\n",
            "1315\n",
            "1318\n",
            "1319\n",
            "1324\n",
            "1329\n",
            "1331\n",
            "1333\n",
            "1346\n",
            "1356\n",
            "1357\n",
            "1359\n",
            "1361\n",
            "1367\n",
            "1371\n",
            "1373\n",
            "1376\n",
            "1384\n",
            "1388\n",
            "1391\n",
            "1398\n",
            "1400\n",
            "1404\n",
            "1414\n",
            "1415\n",
            "1416\n",
            "1417\n",
            "1419\n",
            "1430\n",
            "1431\n",
            "1433\n",
            "1434\n",
            "1437\n",
            "1438\n",
            "1440\n",
            "1442\n",
            "1443\n",
            "1457\n",
            "1458\n",
            "1461\n",
            "1473\n",
            "1482\n",
            "1494\n",
            "1498\n",
            "1500\n",
            "1501\n",
            "1513\n",
            "1517\n",
            "1518\n",
            "1521\n",
            "1522\n",
            "1523\n",
            "1524\n",
            "1525\n",
            "1531\n",
            "1551\n",
            "1570\n",
            "1583\n",
            "1591\n",
            "1598\n",
            "1609\n",
            "1613\n",
            "1616\n",
            "1624\n",
            "1625\n",
            "1631\n",
            "1640\n",
            "1645\n",
            "1648\n",
            "1651\n",
            "1653\n",
            "1654\n",
            "1659\n",
            "1660\n",
            "1664\n",
            "1668\n",
            "1670\n",
            "1672\n",
            "1673\n",
            "1675\n",
            "1678\n",
            "1679\n",
            "1683\n",
            "1684\n",
            "1691\n",
            "1692\n",
            "1693\n",
            "1694\n",
            "1705\n",
            "1718\n",
            "1726\n",
            "1727\n",
            "1731\n",
            "1739\n",
            "1740\n",
            "1742\n",
            "1744\n",
            "1752\n",
            "1769\n",
            "1772\n",
            "1778\n",
            "1781\n",
            "1784\n",
            "1786\n",
            "1788\n",
            "1790\n",
            "1793\n",
            "1795\n",
            "1798\n",
            "1799\n",
            "1807\n",
            "1810\n",
            "1812\n",
            "1825\n",
            "1826\n",
            "1828\n",
            "1830\n",
            "1838\n",
            "1852\n",
            "1853\n",
            "1860\n",
            "1863\n",
            "1865\n",
            "1866\n",
            "1867\n",
            "1883\n",
            "1891\n",
            "1893\n",
            "1895\n",
            "1899\n",
            "1904\n",
            "1913\n",
            "1914\n",
            "1917\n",
            "1918\n",
            "1923\n",
            "1927\n",
            "1929\n",
            "1934\n",
            "1940\n",
            "1941\n",
            "1948\n",
            "1951\n",
            "1953\n",
            "1959\n",
            "1969\n",
            "1970\n",
            "1973\n",
            "1977\n",
            "1983\n",
            "1985\n",
            "1986\n",
            "1988\n",
            "1990\n",
            "1995\n",
            "1998\n",
            "2000\n",
            "2001\n",
            "2002\n",
            "2005\n",
            "2016\n",
            "2020\n",
            "2023\n",
            "2024\n",
            "2039\n",
            "2042\n",
            "2046\n",
            "2051\n",
            "2055\n",
            "2058\n",
            "2064\n",
            "2076\n",
            "2080\n",
            "2083\n",
            "2089\n",
            "2096\n",
            "2098\n",
            "2102\n",
            "2115\n",
            "2123\n",
            "2124\n",
            "2135\n",
            "2136\n",
            "2137\n",
            "2138\n",
            "2139\n",
            "2140\n",
            "2141\n",
            "2145\n",
            "2149\n",
            "2150\n",
            "2151\n",
            "2154\n",
            "2156\n",
            "2157\n",
            "2169\n",
            "2176\n",
            "2180\n",
            "2183\n",
            "2185\n",
            "2187\n",
            "2188\n",
            "2195\n",
            "2199\n",
            "2200\n",
            "2203\n",
            "2204\n",
            "2206\n",
            "2216\n",
            "2218\n",
            "2219\n",
            "2220\n",
            "2232\n",
            "2240\n",
            "2242\n",
            "2251\n",
            "2260\n",
            "2264\n",
            "2279\n",
            "2283\n",
            "2288\n",
            "2289\n",
            "2291\n",
            "2294\n",
            "2299\n",
            "2301\n",
            "2302\n",
            "2308\n",
            "2311\n",
            "2316\n",
            "2319\n",
            "2322\n",
            "2324\n",
            "2327\n",
            "2337\n",
            "2343\n",
            "2344\n",
            "2346\n",
            "2349\n",
            "2350\n",
            "2351\n",
            "2355\n",
            "2379\n",
            "2380\n",
            "2384\n",
            "2388\n",
            "2393\n",
            "2394\n",
            "2396\n",
            "2400\n",
            "2402\n",
            "2406\n",
            "2417\n",
            "2418\n",
            "2427\n",
            "2428\n",
            "2434\n",
            "2443\n",
            "2448\n",
            "2449\n",
            "2451\n",
            "2453\n",
            "2455\n",
            "2463\n",
            "2466\n",
            "2472\n",
            "2473\n",
            "2475\n",
            "2485\n",
            "2492\n",
            "2499\n",
            "2501\n",
            "2506\n",
            "2512\n",
            "2517\n",
            "2528\n",
            "2529\n",
            "2535\n",
            "2536\n",
            "2538\n",
            "2542\n",
            "2549\n",
            "2565\n",
            "2581\n",
            "2582\n",
            "2585\n",
            "2588\n",
            "2591\n",
            "2593\n",
            "2602\n",
            "2607\n",
            "2609\n",
            "2620\n",
            "2621\n",
            "2622\n",
            "2626\n",
            "2632\n",
            "2634\n",
            "2635\n",
            "2651\n",
            "2654\n",
            "2655\n",
            "2656\n",
            "2664\n",
            "2669\n",
            "2673\n",
            "2687\n",
            "2691\n",
            "2694\n",
            "2700\n",
            "2705\n",
            "2708\n",
            "2711\n",
            "2715\n",
            "2719\n",
            "2720\n",
            "2736\n",
            "2740\n",
            "2741\n",
            "2743\n",
            "2744\n",
            "2747\n",
            "2749\n",
            "2753\n",
            "2755\n",
            "2756\n",
            "2758\n",
            "2759\n",
            "2761\n",
            "2763\n",
            "2766\n",
            "2768\n",
            "2775\n",
            "2779\n",
            "2781\n",
            "2785\n",
            "2793\n",
            "2796\n",
            "2800\n",
            "2806\n",
            "2807\n",
            "2808\n",
            "2812\n",
            "2825\n",
            "2826\n",
            "2839\n",
            "2847\n",
            "2848\n",
            "2850\n",
            "2853\n",
            "2857\n",
            "2859\n",
            "2866\n",
            "2871\n",
            "2875\n",
            "2877\n",
            "2886\n",
            "2893\n",
            "2896\n",
            "2897\n",
            "2898\n",
            "2902\n",
            "2905\n",
            "2907\n",
            "2914\n",
            "2916\n",
            "2920\n",
            "2923\n",
            "2940\n",
            "2941\n",
            "2949\n",
            "2950\n",
            "2952\n",
            "2959\n",
            "2965\n",
            "2974\n",
            "2979\n",
            "2989\n",
            "2992\n",
            "2995\n",
            "3007\n",
            "3017\n",
            "3021\n",
            "3027\n",
            "3028\n",
            "3031\n",
            "3035\n",
            "3040\n",
            "3042\n",
            "3044\n",
            "3047\n",
            "3049\n",
            "3050\n",
            "3053\n",
            "3057\n",
            "3066\n",
            "3070\n",
            "3077\n",
            "3078\n",
            "3079\n",
            "3093\n",
            "3100\n",
            "3105\n",
            "3109\n",
            "3112\n",
            "3114\n",
            "3115\n",
            "3122\n",
            "3130\n",
            "3132\n",
            "3134\n",
            "3137\n",
            "3142\n",
            "3144\n",
            "3146\n",
            "3152\n",
            "3155\n",
            "3160\n",
            "3164\n",
            "3166\n",
            "3170\n",
            "3176\n",
            "3177\n",
            "3181\n",
            "3189\n",
            "3190\n",
            "3193\n",
            "3195\n",
            "3198\n",
            "3202\n",
            "3207\n",
            "3210\n",
            "3212\n",
            "3222\n",
            "3224\n",
            "3229\n",
            "3236\n",
            "3237\n",
            "3242\n",
            "3243\n",
            "3246\n",
            "3247\n",
            "3248\n",
            "3249\n",
            "3253\n",
            "3264\n",
            "3274\n",
            "3280\n",
            "3283\n",
            "3287\n",
            "3291\n",
            "3294\n",
            "3295\n",
            "3296\n",
            "3297\n",
            "3305\n",
            "3317\n",
            "3323\n",
            "3327\n",
            "3331\n",
            "3335\n",
            "3336\n",
            "3343\n",
            "3345\n",
            "3348\n",
            "3353\n",
            "3357\n",
            "3359\n",
            "3361\n",
            "3364\n",
            "3370\n",
            "3375\n",
            "3379\n",
            "3381\n",
            "3397\n",
            "3400\n",
            "3404\n",
            "3405\n",
            "3407\n",
            "3410\n",
            "3411\n",
            "3412\n",
            "3417\n",
            "3418\n",
            "3429\n",
            "3430\n",
            "3436\n",
            "3449\n",
            "3462\n",
            "3469\n",
            "3477\n",
            "3478\n",
            "3479\n",
            "3480\n",
            "3488\n",
            "3509\n",
            "3514\n",
            "3517\n",
            "3518\n",
            "3535\n",
            "3537\n",
            "3547\n",
            "3548\n",
            "3557\n",
            "3574\n",
            "3575\n",
            "3580\n",
            "3583\n",
            "3588\n",
            "3589\n",
            "3598\n",
            "3604\n",
            "3606\n",
            "3607\n",
            "3612\n",
            "3613\n",
            "3621\n",
            "3624\n",
            "3625\n",
            "3628\n",
            "3631\n",
            "3640\n",
            "3642\n",
            "3649\n",
            "3650\n",
            "3660\n",
            "3661\n",
            "3664\n",
            "3665\n",
            "3671\n",
            "3672\n",
            "3676\n",
            "3680\n",
            "3683\n",
            "3687\n",
            "3688\n",
            "3689\n",
            "3692\n",
            "3700\n",
            "3701\n",
            "3708\n",
            "3713\n",
            "3714\n",
            "3718\n",
            "3719\n",
            "3720\n",
            "3723\n",
            "3727\n",
            "3730\n",
            "3744\n",
            "3745\n",
            "3746\n",
            "3748\n",
            "3755\n",
            "3759\n",
            "3761\n",
            "3763\n",
            "3766\n",
            "3770\n",
            "3771\n",
            "3772\n",
            "3773\n",
            "3778\n",
            "3780\n",
            "3786\n",
            "3792\n",
            "3796\n",
            "3799\n",
            "3800\n",
            "3805\n",
            "3820\n",
            "3826\n",
            "3828\n",
            "3839\n",
            "3851\n",
            "3859\n",
            "3860\n",
            "3864\n",
            "3865\n",
            "3866\n",
            "3892\n",
            "3898\n",
            "3901\n",
            "3904\n",
            "3907\n",
            "3913\n",
            "3914\n",
            "3921\n",
            "3923\n",
            "3926\n",
            "3936\n",
            "3938\n",
            "3941\n",
            "3945\n",
            "3948\n",
            "3949\n",
            "3952\n",
            "3960\n",
            "3962\n",
            "3963\n",
            "3966\n",
            "3974\n",
            "3979\n",
            "3987\n",
            "3989\n",
            "3995\n",
            "4000\n",
            "4004\n",
            "4012\n",
            "4013\n",
            "4014\n",
            "4024\n",
            "4028\n",
            "4031\n",
            "4037\n",
            "4038\n",
            "4041\n",
            "4045\n",
            "4071\n",
            "4072\n",
            "4080\n",
            "4082\n",
            "4083\n",
            "4084\n",
            "4088\n",
            "4092\n",
            "4094\n",
            "4097\n",
            "4098\n",
            "4102\n",
            "4123\n",
            "4130\n",
            "4146\n",
            "4147\n",
            "4150\n",
            "4154\n",
            "4155\n",
            "4160\n",
            "4161\n",
            "4162\n",
            "4163\n",
            "4168\n",
            "4170\n",
            "4176\n",
            "4178\n",
            "4187\n",
            "4191\n",
            "4199\n",
            "4203\n",
            "4204\n",
            "4205\n",
            "4206\n",
            "4208\n",
            "4221\n",
            "4223\n",
            "4230\n",
            "4236\n",
            "4239\n",
            "4240\n",
            "4243\n",
            "4247\n",
            "4256\n",
            "4257\n",
            "4258\n",
            "4260\n",
            "4266\n",
            "4271\n",
            "4275\n",
            "4289\n",
            "4294\n",
            "4295\n",
            "4306\n",
            "4307\n",
            "4309\n",
            "4310\n",
            "4320\n",
            "4321\n",
            "4323\n",
            "4328\n",
            "4332\n",
            "4333\n",
            "4334\n",
            "4336\n",
            "4338\n",
            "4339\n",
            "4343\n",
            "4344\n",
            "4346\n",
            "4347\n",
            "4350\n",
            "4353\n",
            "4358\n",
            "4359\n",
            "4361\n",
            "4365\n",
            "4367\n",
            "4372\n",
            "4374\n",
            "4375\n",
            "4387\n",
            "4388\n",
            "4401\n",
            "4405\n",
            "4408\n",
            "4409\n",
            "4421\n",
            "4422\n",
            "4424\n",
            "4426\n",
            "4433\n",
            "4455\n",
            "4460\n",
            "4461\n",
            "4463\n",
            "4464\n",
            "4469\n",
            "4478\n",
            "4480\n",
            "4481\n",
            "4484\n",
            "4487\n",
            "4489\n",
            "4491\n",
            "4495\n",
            "4500\n",
            "4503\n",
            "4504\n",
            "4511\n",
            "4518\n",
            "4522\n",
            "4535\n",
            "4547\n",
            "4549\n",
            "4552\n",
            "4560\n",
            "4570\n",
            "4581\n",
            "4588\n",
            "4589\n",
            "4596\n",
            "4599\n",
            "4603\n",
            "4607\n",
            "4609\n",
            "4611\n",
            "4615\n",
            "4619\n",
            "4620\n",
            "4623\n",
            "4632\n",
            "4634\n",
            "4637\n",
            "4638\n",
            "4639\n",
            "4640\n",
            "4643\n",
            "4647\n",
            "4652\n",
            "4654\n",
            "4660\n",
            "4668\n",
            "4670\n",
            "4672\n",
            "4676\n",
            "4679\n",
            "4682\n",
            "4686\n",
            "4692\n",
            "4699\n",
            "4702\n",
            "4703\n",
            "4707\n",
            "4709\n",
            "4711\n",
            "4712\n",
            "4715\n",
            "4722\n",
            "4723\n",
            "4728\n",
            "4735\n",
            "4736\n",
            "4740\n",
            "4742\n",
            "4759\n",
            "4762\n",
            "4763\n",
            "4765\n",
            "4768\n",
            "4771\n",
            "4781\n",
            "4787\n",
            "4789\n",
            "4796\n",
            "4800\n",
            "4805\n",
            "4815\n",
            "4820\n",
            "4826\n",
            "4831\n",
            "4834\n",
            "4835\n",
            "4837\n",
            "4840\n",
            "4846\n",
            "4854\n",
            "4858\n",
            "4860\n",
            "4863\n",
            "4868\n",
            "4870\n",
            "4871\n",
            "4873\n",
            "4877\n",
            "4878\n",
            "4884\n",
            "4889\n",
            "4894\n",
            "4895\n",
            "4897\n",
            "4903\n",
            "4906\n",
            "4911\n",
            "4914\n",
            "4915\n",
            "4921\n",
            "4922\n",
            "4924\n",
            "4926\n",
            "4928\n",
            "4933\n",
            "4940\n",
            "4945\n",
            "4946\n",
            "4954\n",
            "4957\n",
            "4959\n",
            "4969\n",
            "4981\n",
            "4984\n",
            "4990\n",
            "4991\n",
            "4995\n",
            "4997\n",
            "5005\n",
            "5017\n",
            "5020\n",
            "5021\n",
            "5029\n",
            "5037\n",
            "5038\n",
            "5040\n",
            "5051\n",
            "5055\n",
            "5056\n",
            "5057\n",
            "5061\n",
            "5064\n",
            "5069\n",
            "5073\n",
            "5076\n",
            "5077\n",
            "5082\n",
            "5083\n",
            "5087\n",
            "5088\n",
            "5090\n",
            "5092\n",
            "5099\n",
            "5104\n",
            "5117\n",
            "5131\n",
            "5134\n",
            "5135\n",
            "5136\n",
            "5151\n",
            "5155\n",
            "5159\n",
            "5163\n",
            "5169\n",
            "5179\n",
            "5180\n",
            "5181\n",
            "5189\n",
            "5191\n",
            "5192\n",
            "5193\n",
            "5194\n",
            "5198\n",
            "5200\n",
            "5203\n",
            "5204\n",
            "5210\n",
            "5213\n",
            "5218\n",
            "5219\n",
            "5227\n",
            "5231\n",
            "5252\n",
            "5254\n",
            "5261\n",
            "5264\n",
            "5268\n",
            "5271\n",
            "5273\n",
            "5275\n",
            "5278\n",
            "5281\n",
            "5286\n",
            "5288\n",
            "5290\n",
            "5299\n",
            "5302\n",
            "5304\n",
            "5305\n",
            "5306\n",
            "5312\n",
            "5320\n",
            "5325\n",
            "5328\n",
            "5331\n",
            "5332\n",
            "5338\n",
            "5352\n",
            "5353\n",
            "5355\n",
            "5360\n",
            "5368\n",
            "5370\n",
            "5378\n",
            "5389\n",
            "5405\n",
            "5411\n",
            "5415\n",
            "5416\n",
            "5422\n",
            "5441\n",
            "5450\n",
            "5454\n",
            "5455\n",
            "5463\n",
            "5464\n",
            "5465\n",
            "5466\n",
            "5467\n",
            "5473\n",
            "5478\n",
            "5479\n",
            "5485\n",
            "5486\n",
            "5493\n",
            "5494\n",
            "5499\n",
            "5502\n",
            "5505\n",
            "5512\n",
            "5516\n",
            "5517\n",
            "5519\n",
            "5521\n",
            "5527\n",
            "5528\n",
            "5531\n",
            "5539\n",
            "5544\n",
            "5546\n",
            "5552\n",
            "5556\n",
            "5560\n",
            "5574\n",
            "5575\n",
            "5580\n",
            "5582\n",
            "5593\n",
            "5603\n",
            "5606\n",
            "5609\n",
            "5618\n",
            "5620\n",
            "5625\n",
            "5646\n",
            "5647\n",
            "5652\n",
            "5658\n",
            "5668\n",
            "5678\n",
            "5680\n",
            "5682\n",
            "5688\n",
            "5689\n",
            "5690\n",
            "5691\n",
            "5693\n",
            "5698\n",
            "5704\n",
            "5710\n",
            "5711\n",
            "5715\n",
            "5716\n",
            "5720\n",
            "5723\n",
            "5735\n",
            "5743\n",
            "5745\n",
            "5746\n",
            "5749\n",
            "5753\n",
            "5756\n",
            "5759\n",
            "5761\n",
            "5767\n",
            "5774\n",
            "5775\n",
            "5785\n",
            "5790\n",
            "5792\n",
            "5793\n",
            "5797\n",
            "5806\n",
            "5808\n",
            "5810\n",
            "5817\n",
            "5819\n",
            "5830\n",
            "5838\n",
            "5849\n",
            "5850\n",
            "5852\n",
            "5853\n",
            "5855\n",
            "5857\n",
            "5864\n",
            "5865\n",
            "5881\n",
            "5883\n",
            "5884\n",
            "5885\n",
            "5896\n",
            "5901\n",
            "5902\n",
            "5911\n",
            "5920\n",
            "5922\n",
            "5928\n",
            "5929\n",
            "5938\n",
            "5940\n",
            "5941\n",
            "5960\n",
            "5967\n",
            "5968\n",
            "5970\n",
            "5974\n",
            "5980\n",
            "5981\n",
            "5989\n",
            "5995\n",
            "5997\n",
            "6004\n",
            "6012\n",
            "6022\n",
            "6026\n",
            "6029\n",
            "6037\n",
            "6055\n",
            "6059\n",
            "6060\n",
            "6067\n",
            "6070\n",
            "6076\n",
            "6077\n",
            "6100\n",
            "6106\n",
            "6123\n",
            "6134\n",
            "6145\n",
            "6146\n",
            "6149\n",
            "6150\n",
            "6151\n",
            "6158\n",
            "6161\n",
            "6164\n",
            "6169\n",
            "6170\n",
            "6172\n",
            "6175\n",
            "6177\n",
            "6180\n",
            "6184\n",
            "6186\n",
            "6187\n",
            "6190\n",
            "6193\n",
            "6207\n",
            "6208\n",
            "6209\n",
            "6210\n",
            "6213\n",
            "6216\n",
            "6218\n",
            "6219\n",
            "6227\n",
            "6232\n",
            "6233\n",
            "6238\n",
            "6240\n",
            "6241\n",
            "6243\n",
            "6244\n",
            "6249\n",
            "6251\n",
            "6253\n",
            "6260\n",
            "6264\n",
            "6283\n",
            "6285\n",
            "6287\n",
            "6288\n",
            "6299\n",
            "6303\n",
            "6308\n",
            "6309\n",
            "6315\n",
            "6316\n",
            "6317\n",
            "6318\n",
            "6326\n",
            "6327\n",
            "6333\n",
            "6345\n",
            "6350\n",
            "6353\n",
            "6355\n",
            "6360\n",
            "6370\n",
            "6372\n",
            "6390\n",
            "6391\n",
            "6393\n",
            "6394\n",
            "6405\n",
            "6410\n",
            "6412\n",
            "6422\n",
            "6424\n",
            "6429\n",
            "6433\n",
            "6436\n",
            "6437\n",
            "6449\n",
            "6454\n",
            "6460\n",
            "6465\n",
            "6470\n",
            "6481\n",
            "6483\n",
            "6487\n",
            "6496\n",
            "6501\n",
            "6502\n",
            "6509\n",
            "6510\n",
            "6511\n",
            "6513\n",
            "6516\n",
            "6523\n",
            "6524\n",
            "6530\n",
            "6533\n",
            "6537\n",
            "6544\n",
            "6546\n",
            "6547\n",
            "6553\n",
            "6560\n",
            "6562\n",
            "6564\n",
            "6568\n",
            "6569\n",
            "6573\n",
            "6574\n",
            "6576\n",
            "6584\n",
            "6590\n",
            "6591\n",
            "6592\n",
            "6594\n",
            "6601\n",
            "6604\n",
            "6607\n",
            "6613\n",
            "6614\n",
            "6621\n",
            "6622\n",
            "6623\n",
            "6625\n",
            "6628\n",
            "6631\n",
            "6632\n",
            "6637\n",
            "6642\n",
            "6644\n",
            "6649\n",
            "6650\n",
            "6651\n",
            "6659\n",
            "6660\n",
            "6667\n",
            "6669\n",
            "6674\n",
            "6675\n",
            "6682\n",
            "6689\n",
            "6691\n",
            "6695\n",
            "6709\n",
            "6716\n",
            "6723\n",
            "6728\n",
            "6730\n",
            "6737\n",
            "6741\n",
            "6747\n",
            "6751\n",
            "6755\n",
            "6757\n",
            "6758\n",
            "6767\n",
            "6775\n",
            "6781\n",
            "6783\n",
            "6784\n",
            "6791\n",
            "6794\n",
            "6800\n",
            "6805\n",
            "6810\n",
            "6817\n",
            "6820\n",
            "6822\n",
            "6827\n",
            "6833\n",
            "6841\n",
            "6848\n",
            "6855\n",
            "6863\n",
            "6864\n",
            "6867\n",
            "6874\n",
            "6883\n",
            "6884\n",
            "6887\n",
            "6890\n",
            "6891\n",
            "6896\n",
            "6900\n",
            "6913\n",
            "6916\n",
            "6917\n",
            "6918\n",
            "6919\n",
            "6922\n",
            "6923\n",
            "6926\n",
            "6934\n",
            "6942\n",
            "6949\n",
            "6952\n",
            "6953\n",
            "6957\n",
            "6962\n",
            "6966\n",
            "6968\n",
            "6969\n",
            "6976\n",
            "6983\n",
            "6984\n",
            "6987\n",
            "6993\n",
            "7000\n",
            "7003\n",
            "7006\n",
            "7007\n",
            "7008\n",
            "7012\n",
            "7018\n",
            "7024\n",
            "7030\n",
            "7031\n",
            "7032\n",
            "7033\n",
            "7034\n",
            "7039\n",
            "7041\n",
            "7043\n",
            "7047\n",
            "7048\n",
            "7051\n",
            "7056\n",
            "7062\n",
            "7065\n",
            "7076\n",
            "7079\n",
            "7080\n",
            "7083\n",
            "7084\n",
            "7086\n",
            "7088\n",
            "7089\n",
            "7095\n",
            "7099\n",
            "7104\n",
            "7108\n",
            "7110\n",
            "7113\n",
            "7116\n",
            "7119\n",
            "7121\n",
            "7123\n",
            "7127\n",
            "7130\n",
            "7133\n",
            "7134\n",
            "7136\n",
            "7139\n",
            "7140\n",
            "7146\n",
            "7148\n",
            "7154\n",
            "7156\n",
            "7161\n",
            "7163\n",
            "7166\n",
            "7169\n",
            "7175\n",
            "7178\n",
            "7184\n",
            "7185\n",
            "7191\n",
            "7196\n",
            "7203\n",
            "7204\n",
            "7213\n",
            "7216\n",
            "7222\n",
            "7225\n",
            "7226\n",
            "7227\n",
            "7230\n",
            "7231\n",
            "7236\n",
            "7241\n",
            "7242\n",
            "7244\n",
            "7245\n",
            "7246\n",
            "7247\n",
            "7258\n",
            "7260\n",
            "7262\n",
            "7266\n",
            "7269\n",
            "7275\n",
            "7277\n",
            "7281\n",
            "7283\n",
            "7288\n",
            "7291\n",
            "7296\n",
            "7297\n",
            "7298\n",
            "7300\n",
            "7304\n",
            "7305\n",
            "7313\n",
            "7317\n",
            "7321\n",
            "7322\n",
            "7324\n",
            "7327\n",
            "7329\n",
            "7330\n",
            "7345\n",
            "7348\n",
            "7350\n",
            "7351\n",
            "7356\n",
            "7357\n",
            "7359\n",
            "7361\n",
            "7371\n",
            "7373\n",
            "7374\n",
            "7376\n",
            "7377\n",
            "7385\n",
            "7389\n",
            "7393\n",
            "7397\n",
            "7403\n",
            "7404\n",
            "7405\n",
            "7406\n",
            "7411\n",
            "7412\n",
            "7413\n",
            "7419\n",
            "7420\n",
            "7432\n",
            "7433\n",
            "7436\n",
            "7438\n",
            "7441\n",
            "7442\n",
            "7446\n",
            "7447\n",
            "7448\n",
            "7460\n",
            "7461\n",
            "7465\n",
            "7467\n",
            "7472\n",
            "7474\n",
            "7479\n",
            "7482\n",
            "7483\n",
            "7486\n",
            "7490\n",
            "7504\n",
            "7508\n",
            "7509\n",
            "7513\n",
            "7520\n",
            "7521\n",
            "7535\n",
            "7537\n",
            "7538\n",
            "7542\n",
            "7545\n",
            "7551\n",
            "7552\n",
            "7555\n",
            "7557\n",
            "7559\n",
            "7562\n",
            "7580\n",
            "7581\n",
            "7583\n",
            "7585\n",
            "7587\n",
            "7589\n",
            "7590\n",
            "7592\n",
            "7594\n",
            "7595\n",
            "7599\n",
            "7603\n",
            "7611\n",
            "7617\n",
            "7618\n",
            "7619\n",
            "7622\n",
            "7626\n",
            "7628\n",
            "7629\n",
            "7632\n",
            "7643\n",
            "7645\n",
            "7647\n",
            "7648\n",
            "7651\n",
            "7656\n",
            "7659\n",
            "7663\n",
            "7668\n",
            "7670\n",
            "7673\n",
            "7680\n",
            "7685\n",
            "7688\n",
            "7696\n",
            "7698\n",
            "7703\n",
            "7705\n",
            "7712\n",
            "7714\n",
            "7715\n",
            "7724\n",
            "7726\n",
            "7734\n",
            "7735\n",
            "7739\n",
            "7740\n",
            "7748\n",
            "7749\n",
            "7750\n",
            "7751\n",
            "7754\n",
            "7757\n",
            "7760\n",
            "7762\n",
            "7765\n",
            "7769\n",
            "7780\n",
            "7785\n",
            "7792\n",
            "7793\n",
            "7795\n",
            "7798\n",
            "7799\n",
            "7801\n",
            "7805\n",
            "7807\n",
            "7808\n",
            "7813\n",
            "7817\n",
            "7822\n",
            "7825\n",
            "7826\n",
            "7827\n",
            "7829\n",
            "7833\n",
            "7844\n",
            "7847\n",
            "7848\n",
            "7858\n",
            "7859\n",
            "7865\n",
            "7867\n",
            "7870\n",
            "7874\n",
            "7878\n",
            "7880\n",
            "7884\n",
            "7887\n",
            "7888\n",
            "7891\n",
            "7898\n",
            "7903\n",
            "7905\n",
            "7912\n",
            "7919\n",
            "7921\n",
            "7924\n",
            "7933\n",
            "7942\n",
            "7945\n",
            "7946\n",
            "7947\n",
            "7952\n",
            "7953\n",
            "7955\n",
            "7967\n",
            "7968\n",
            "7973\n",
            "7976\n",
            "7983\n",
            "7984\n",
            "7991\n",
            "7999\n",
            "8004\n",
            "8008\n",
            "8022\n",
            "8024\n",
            "8026\n",
            "8027\n",
            "8029\n",
            "8031\n",
            "8036\n",
            "8038\n",
            "8045\n",
            "8047\n",
            "8049\n",
            "8050\n",
            "8060\n",
            "8072\n",
            "8087\n",
            "8089\n",
            "8095\n",
            "8097\n",
            "8100\n",
            "8105\n",
            "8112\n",
            "8117\n",
            "8121\n",
            "8123\n",
            "8127\n",
            "8129\n",
            "8131\n",
            "8138\n",
            "8143\n",
            "8148\n",
            "8149\n",
            "8153\n",
            "8158\n",
            "8161\n",
            "8162\n",
            "8166\n",
            "8167\n",
            "8168\n",
            "8169\n",
            "8176\n",
            "8181\n",
            "8184\n",
            "8189\n",
            "8194\n",
            "8198\n",
            "8204\n",
            "8208\n",
            "8221\n",
            "8222\n",
            "8227\n",
            "8235\n",
            "8245\n",
            "8253\n",
            "8260\n",
            "8262\n",
            "8274\n",
            "8277\n",
            "8278\n",
            "8281\n",
            "8288\n",
            "8294\n",
            "8295\n",
            "8297\n",
            "8302\n",
            "8308\n",
            "8330\n",
            "8333\n",
            "8335\n",
            "8338\n",
            "8341\n",
            "8343\n",
            "8344\n",
            "8345\n",
            "8348\n",
            "8349\n",
            "8351\n",
            "8354\n",
            "8359\n",
            "8360\n",
            "8367\n",
            "8373\n",
            "8375\n",
            "8376\n",
            "8379\n",
            "8382\n",
            "8385\n",
            "8388\n",
            "8398\n",
            "8402\n",
            "8403\n",
            "8408\n",
            "8409\n",
            "8416\n",
            "8428\n",
            "8432\n",
            "8433\n",
            "8435\n",
            "8441\n",
            "8452\n",
            "8453\n",
            "8455\n",
            "8466\n",
            "8469\n",
            "8474\n",
            "8477\n",
            "8481\n",
            "8487\n",
            "8492\n",
            "8506\n",
            "8508\n",
            "8514\n",
            "8522\n",
            "8523\n",
            "8530\n",
            "8534\n",
            "8538\n",
            "8539\n",
            "8540\n",
            "8550\n",
            "8565\n",
            "8568\n",
            "8571\n",
            "8578\n",
            "8579\n",
            "8582\n",
            "8585\n",
            "8588\n",
            "8595\n",
            "8606\n",
            "8612\n",
            "8613\n",
            "8614\n",
            "8619\n",
            "8628\n",
            "8634\n",
            "8635\n",
            "8638\n",
            "8642\n",
            "8643\n",
            "8649\n",
            "8650\n",
            "8651\n",
            "8663\n",
            "8666\n",
            "8687\n",
            "8698\n",
            "8700\n",
            "8706\n",
            "8708\n",
            "8711\n",
            "8713\n",
            "8716\n",
            "8718\n",
            "8729\n",
            "8731\n",
            "8734\n",
            "8735\n",
            "8740\n",
            "8743\n",
            "8744\n",
            "8752\n",
            "8759\n",
            "8761\n",
            "8765\n",
            "8776\n",
            "8778\n",
            "8779\n",
            "8782\n",
            "8785\n",
            "8787\n",
            "8791\n",
            "8798\n",
            "8804\n",
            "8808\n",
            "8811\n",
            "8815\n",
            "8824\n",
            "8827\n",
            "8831\n",
            "8834\n",
            "8840\n",
            "8842\n",
            "8850\n",
            "8852\n",
            "8854\n",
            "8857\n",
            "8865\n",
            "8866\n",
            "8868\n",
            "8870\n",
            "8873\n",
            "8893\n",
            "8897\n",
            "8900\n",
            "8906\n",
            "8910\n",
            "8911\n",
            "8912\n",
            "8917\n",
            "8919\n",
            "8921\n",
            "8925\n",
            "8928\n",
            "8940\n",
            "8943\n",
            "8946\n",
            "8947\n",
            "8948\n",
            "8950\n",
            "8952\n",
            "8963\n",
            "8964\n",
            "8965\n",
            "8966\n",
            "8967\n",
            "8969\n",
            "8975\n",
            "8979\n",
            "8983\n",
            "8985\n",
            "8988\n",
            "8997\n",
            "8998\n",
            "9001\n",
            "9006\n",
            "9010\n",
            "9011\n",
            "9012\n",
            "9023\n",
            "9025\n",
            "9028\n",
            "9029\n",
            "9032\n",
            "9041\n",
            "9053\n",
            "9059\n",
            "9065\n",
            "9069\n",
            "9075\n",
            "9076\n",
            "9085\n",
            "9086\n",
            "9088\n",
            "9091\n",
            "9093\n",
            "9095\n",
            "9099\n",
            "9102\n",
            "9110\n",
            "9114\n",
            "9118\n",
            "9119\n",
            "9122\n",
            "9125\n",
            "9128\n",
            "9129\n",
            "9131\n",
            "9134\n",
            "9142\n",
            "9148\n",
            "9150\n",
            "9152\n",
            "9154\n",
            "9156\n",
            "9159\n",
            "9164\n",
            "9166\n",
            "9174\n",
            "9177\n",
            "9179\n",
            "9183\n",
            "9184\n",
            "9187\n",
            "9194\n",
            "9195\n",
            "9197\n",
            "9200\n",
            "9205\n",
            "9207\n",
            "9208\n",
            "9212\n",
            "9220\n",
            "9221\n",
            "9222\n",
            "9228\n",
            "9232\n",
            "9237\n",
            "9244\n",
            "9254\n",
            "9270\n",
            "9277\n",
            "9283\n",
            "9287\n",
            "9295\n",
            "9296\n",
            "9298\n",
            "9302\n",
            "9310\n",
            "9311\n",
            "9319\n",
            "9320\n",
            "9330\n",
            "9347\n",
            "9355\n",
            "9361\n",
            "9362\n",
            "9372\n",
            "9374\n",
            "9378\n",
            "9390\n",
            "9391\n",
            "9393\n",
            "9395\n",
            "9396\n",
            "9397\n",
            "9399\n",
            "9400\n",
            "9414\n",
            "9415\n",
            "9417\n",
            "9419\n",
            "9421\n",
            "9427\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DataPrep"
      ],
      "metadata": {
        "id": "dinTbUVgHB4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataPrep:\n",
        "\n",
        "  \"\"\"\n",
        "    The role of this class is to prepare the raw documents, it has three functions:\n",
        "      * clean_text_data\n",
        "      * create_text_data\n",
        "      * get_new_position\n",
        "      detailled bellow\n",
        "  \"\"\"\n",
        "\n",
        "#perhaps create a function that actually check for other characters that will be tokenized into whitespace or other special characters\n",
        "\n",
        "  def clean_text_data(text_data):\n",
        "\n",
        "    \"\"\"\n",
        "      the role of this function is to get rid of the special charcters that are either tokenized into whitespace like '\\xad',\n",
        "      or that will take space into the tokens vector like the succession of '===' where each one is a single token\n",
        "      Every whitespace will be left to the tokenizer to deal with and the following code is set based on that\n",
        "    \"\"\"\n",
        "\n",
        "    import re\n",
        "    replacements = [\n",
        "        (r'\\.{2,}', ' '),\n",
        "        (r'\\={2,}', ' '),\n",
        "        (r'\\-{2,}', ' '),\n",
        "        (r'\\xad', ' '),\n",
        "        (r'\\xa0', ' '),\n",
        "        (r'\\x80', ''),\n",
        "        (r'\\x9d', ''),\n",
        "        (r'\\x13', ' '),\n",
        "        ]\n",
        "\n",
        "    for old, new in replacements:\n",
        "      text_data = re.sub(old, new, text_data)\n",
        "    return text_data\n",
        "\n",
        "\n",
        "  def create_text_data(raw_json_data):\n",
        "\n",
        "    \"\"\"\n",
        "      this function creates a vector for each document using the cleaning function\n",
        "    \"\"\"\n",
        "\n",
        "    the_text_data = []\n",
        "    for line in raw_json_data:\n",
        "      # if line['annotations'] != [{'result': []}]:   # the document has annotations  #in fact, we'll clean the whole data and leave the test to be done later\n",
        "      the_text_data.append(DataPrep.clean_text_data(line['data']['text']))\n",
        "    return the_text_data\n",
        "\n",
        "  def get_new_position(old_text, new_text, pos):\n",
        "    import re\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "      this function will allow to find the position of a special entity in the new cleaned text using the position in the raw text given in annotations\n",
        "    \"\"\"\n",
        "\n",
        "    if new_text[pos] == old_text[pos]: #pos did not change\n",
        "      return pos\n",
        "\n",
        "    else: # we actually need to update the pos\n",
        "\n",
        "      special_entity = old_text[pos]\n",
        "      if DataPrep.clean_text_data(special_entity).split() == []:\n",
        "        # print(special_entity, pos)\n",
        "        return 'not so special entity'      #unnecessary prints, todo: remove this print, it's basically used to test the output later: type ?= slice\n",
        "\n",
        "      if special_entity[0:2] == '--':\n",
        "        special_entity = special_entity[2:] #cases where the NE starts with -, preceeded by -- in data['text'] and omitted by the cleaninh function\n",
        "        pos = slice(pos.start + 2, pos.stop)\n",
        "\n",
        "      if special_entity[0:1] == '-' and special_entity[1:2] != '-':\n",
        "        special_entity = special_entity[1:] #cases where the NE starts with -, preceeded by -- in data['text'] and omitted by the cleaninh function\n",
        "        pos = slice(pos.start + 1, pos.stop)\n",
        "\n",
        "      in_old_text = [m.start() for m in re.finditer(re.escape(special_entity), old_text)]\n",
        "      in_new_text = [m.start() for m in re.finditer(re.escape(DataPrep.clean_text_data(special_entity)), new_text)]\n",
        "\n",
        "      \"\"\"\n",
        "        the previous lists take into consideration the possibility of having a special entity with more than on occurence,\n",
        "        so they'll make sure to align the occurences in the raw data and in the clean version\n",
        "      \"\"\"\n",
        "\n",
        "      if len(in_new_text) != len(in_old_text) and special_entity != '.':  #written differently #with whitespaces, or special characters\n",
        "        # this block is for test, and shouldn't be achieved\n",
        "        # print(indice, \"\\n not same len\")      #unnecessary prints, todo: remove this print\n",
        "        print('\\t special_entity: ', special_entity, 'at pos', pos)\n",
        "        print('in_old_text: ', in_old_text, '\\t in_new_text: ',in_new_text)\n",
        "\n",
        "      else:\n",
        "\n",
        "        all_occ = dict(zip(in_old_text,in_new_text))\n",
        "\n",
        "        start = all_occ[pos.start]\n",
        "\n",
        "        end = start+len(DataPrep.clean_text_data(old_text[pos]))#-1\n",
        "\n",
        "        # check\n",
        "        if DataPrep.clean_text_data(old_text[pos]) != new_text[slice(start, end)]:\n",
        "          print('not similar', old_text[pos], '\\t', new_text[slice(start, end)])  #unnecessary prints, todo: remove this print\n",
        "\n",
        "\n",
        "        return slice(start,end)\n",
        "\n",
        "\n",
        "  # dev_text_data = create_text_data(dev_data)\n",
        "  # train_text_data =create_text_data(json_data)"
      ],
      "metadata": {
        "id": "Pj2JBwTz86mR"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_text_data = DataPrep.create_text_data(train_data)\n",
        "dev_text_data = DataPrep.create_text_data(dev_data)\n",
        "train_text_data[0]"
      ],
      "metadata": {
        "id": "g1cVIg-KxdNO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "fe0324a3-2e2a-4dc5-cd4b-e6c1e06571d7"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\n(7) On specific query by the Bench about an entry of Rs. 1,31,37,500 on deposit side of Hongkong Bank account of which a photo copy is appearing at p. 40 of assessee's paper book, learned authorised representative submitted that it was related to loan from broker, Rahul & Co. on the basis of his submission a necessary mark is put by us on that photo copy.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_model_tokenize(model_name, train_text_data, dev_text_data ):\n",
        "  #set model (name) & tokenize train and dev data\n",
        "  model = model_name\n",
        "  from transformers import AutoTokenizer\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "\n",
        "  train_tokenized = tokenizer(train_text_data,  is_split_into_words = False, return_offsets_mapping = True, add_special_tokens = True, truncation = True, padding = 'max_length', max_length=512, stride=128, return_overflowing_tokens=True,\n",
        "  )  #when text is not splitted, the positions in str are accurate\n",
        "\n",
        "  train_tokens = [tokenizer.convert_ids_to_tokens(tokenized) for tokenized in train_tokenized.input_ids]\n",
        "\n",
        "  dev_tokenized = tokenizer(dev_text_data,  is_split_into_words = False,  return_offsets_mapping = True, add_special_tokens = True,  padding = 'max_length', truncation = True, max_length=512, stride=128, return_overflowing_tokens=True, )  #when text is not splitted, the positions are accurate\n",
        "\n",
        "  dev_tokens = [tokenizer.convert_ids_to_tokens(tokenized) for tokenized in dev_tokenized.input_ids]\n",
        "\n",
        "  return train_tokenized, dev_tokenized, train_tokens, dev_tokens\n"
      ],
      "metadata": {
        "id": "yHIY81mP86mX"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_token_pos(mappers, start_str, end_str):\n",
        "  # get position in tokens list from annotations\n",
        "  for idx, (start, end) in enumerate(mappers):\n",
        "    if idx<= mappers.index(max(mappers)):\n",
        "        if start <= start_str:\n",
        "            idx_start = idx\n",
        "        if end == end_str:\n",
        "            idx_end = idx+1\n",
        "        if end < end_str:\n",
        "            idx_end = idx+2\n",
        "\n",
        "  pos = slice(idx_start, idx_end)\n",
        "  return pos"
      ],
      "metadata": {
        "id": "QAmHRPCyA8WG"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I still need to include cases where there are no annotations, this wouldn't be aligned actually because\n",
        "# I think I did with         # if result != []:   but still need to check the alignment  too\n",
        "\n",
        "def affect(raw_json_data, the_tokenized, the_text_data, the_tokens):\n",
        "\n",
        "  the_tokens_labels = [['O' for mapper in the_tokenized[\"offset_mapping\"][i]] for i in range(len(the_tokenized[\"offset_mapping\"]))]\n",
        "\n",
        "  for indice in range(len(raw_json_data)):\n",
        "    # print(indice)\n",
        "    tkn_indice = the_tokenized['overflow_to_sample_mapping'].index(indice)\n",
        "    # print('tkn_indice: ', tkn_indice)\n",
        "\n",
        "    for annot in raw_json_data[indice]['annotations']: #list of all the entities that are part of the deal\n",
        "      if annot != [{'result': []}]: #is there a risk of mismatch with the txt data prep, append\n",
        "        for result in annot['result']:    #how does it deal with the empty ones\n",
        "          deb_str = result['value']['start']\n",
        "          fin_str = result['value']['end']\n",
        "          label = result['value']['labels'][0]\n",
        "\n",
        "          raw_text = raw_json_data[indice]['data']['text']\n",
        "          special_entity = result['value']['text']\n",
        "          clean_pos = DataPrep.get_new_position(raw_text,the_text_data[indice], slice(deb_str,fin_str) ) #will give the tupple for the offset_map\n",
        "\n",
        "          if type(clean_pos) == slice:\n",
        "\n",
        "            if the_tokenized['overflow_to_sample_mapping'].count(indice) > 1:\n",
        "\n",
        "            #split into chunks: we need to check into both using offset mapping for example\n",
        "              for i in range(the_tokenized['overflow_to_sample_mapping'].count(indice)+1) :\n",
        "                if clean_pos.start > the_tokenized[\"offset_mapping\"][tkn_indice+i+1][2][0] and clean_pos.stop < max(the_tokenized[\"offset_mapping\"][tkn_indice+i+1])[1]:\n",
        "\n",
        "                  mapper = the_tokenized[\"offset_mapping\"][tkn_indice+i+1]\n",
        "                  token_pos =  get_token_pos(mapper, clean_pos.start, clean_pos.stop)\n",
        "\n",
        "                  the_tokens_labels[tkn_indice+i+1][token_pos.start] = 'B-'+label\n",
        "\n",
        "                  for x in range(token_pos.start+1, token_pos.stop):\n",
        "                    if x <len(the_tokens_labels[tkn_indice+i+1]):\n",
        "                      the_tokens_labels[tkn_indice+i+1][x] = 'I-'+label\n",
        "\n",
        "\n",
        "                if clean_pos.start > the_tokenized[\"offset_mapping\"][tkn_indice+i][2][0] and clean_pos.stop < max(the_tokenized[\"offset_mapping\"][tkn_indice+i])[1]:\n",
        "                  #update the clean_pos too\n",
        "                  tkn_indice += i\n",
        "                  break\n",
        "\n",
        "            mapper = the_tokenized[\"offset_mapping\"][tkn_indice]\n",
        "            token_pos =  get_token_pos(mapper, clean_pos.start, clean_pos.stop)\n",
        "\n",
        "            #check similarity\n",
        "\n",
        "            if ''.join(the_tokens[tkn_indice][token_pos]).replace('##', '').replace('', '').replace('', '') != ''.join(the_text_data[indice][clean_pos].split()) and abs(len(''.join(the_tokens[tkn_indice][token_pos]).replace('##', '').replace('', '').replace('', ''))-len(''.join(the_text_data[indice][clean_pos].split())))>5:\n",
        "              #the length check was added, because sometimes the tokenized might include what seems to be a part of the special entity like ., the year in a date, ).\n",
        "              print('here?!', indice,tkn_indice, special_entity, the_tokens[tkn_indice][token_pos], '\\t', token_pos, '\\t', slice(deb_str,fin_str),  clean_pos )\n",
        "\n",
        "\n",
        "            the_tokens_labels[tkn_indice][token_pos.start] = 'B-'+label\n",
        "\n",
        "            for x in range(token_pos.start+1, token_pos.stop):\n",
        "              if x <len(the_tokens_labels[tkn_indice]):\n",
        "                the_tokens_labels[tkn_indice][x] = 'I-'+label\n",
        "\n",
        "              else:\n",
        "                if x!=512 :\n",
        "                  print('not simillar,  \\t', 'indice: \\t', indice, '\\t x: \\t', x )\n",
        "\n",
        "          else:\n",
        "            print('not so special entity')\n",
        "\n",
        "# tokens_labels = affect(tokenized, text_data, json_data, tokens):\n",
        "# dev_tokens_labels = affect(dev_tokenized, dev_text_data, dev_data, dev_tokens):\n"
      ],
      "metadata": {
        "id": "RVqeyPEDBAsu"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### draft"
      ],
      "metadata": {
        "id": "Gf2E9OkhThEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model_name = 'roberta-base'\n",
        "model_name = 'dslim/bert-base-NER'\n",
        "train_tokenized, dev_tokenized, train_tokens, dev_tokens = set_model_tokenize(model_name, train_text_data, dev_text_data )\n"
      ],
      "metadata": {
        "id": "1PTHfM-cJnvm"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'roberta-base'\n",
        "# model_name = 'dslim/bert-base-NER'\n",
        "rob_train_tokenized, rob_dev_tokenized, rob_train_tokens, rob_dev_tokens = set_model_tokenize(model_name, train_text_data, dev_text_data )\n"
      ],
      "metadata": {
        "id": "baMdNFXdLLpz"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_tokenized.keys()  #[tokenizer.convert_ids_to_tokens(tokenized) for tokenized in train_tokenized.input_ids]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSQnY1GYKyw6",
        "outputId": "b78ffcf9-6633-43c9-bcb9-93da056bff75"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping'])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_tokens[0][:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1o_Y_tPK0gY",
        "outputId": "2d6c5aaf-a1f5-42ae-acc4-a7489b508aa0"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]', '(', '7', ')', 'On', 'specific', 'que', '##ry', 'by', 'the']"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rob_train_tokens[0][:10]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TY1s76WELV_o",
        "outputId": "1306e509-5790-4dea-84be-51628b19918a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s>', '', '', '(', '7', ')', 'On', 'specific', 'query', 'by']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(train_tokenized['input_ids'][0][:100])\n",
        "train_tokenized.keys()\n",
        "for tokenized in train_tokenized['input_ids'][0][:10]:\n",
        "  print(tokenized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enKJ1mYIOCl5",
        "outputId": "926c1c33-0ab1-4380-dc9f-116aaeab510e"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "101\n",
            "113\n",
            "128\n",
            "114\n",
            "1212\n",
            "2747\n",
            "15027\n",
            "1616\n",
            "1118\n",
            "1103\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print([AutoTokenizer.from_pretrained('roberta-base').convert_ids_to_tokens(tokenized) for tokenized in rob_train_tokenized['input_ids'][0][:100]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCEWAM2BNLPX",
        "outputId": "6d3da9b2-cc6d-4272-ac3c-9dfbc992da9f"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<s>', '', '', '(', '7', ')', 'On', 'specific', 'query', 'by', 'the', 'Bench', 'about', 'an', 'entry', 'of', 'Rs', '.', '1', ',', '31', ',', '37', ',', '500', 'on', 'deposit', 'side', 'of', 'Hong', 'k', 'ong', 'Bank', 'account', 'of', 'which', 'a', 'photo', 'copy', 'is', 'appearing', 'at', 'p', '.', '40', 'of', 'ass', 'essee', \"'s\", 'paper', 'book', ',', 'learned', 'authorised', 'representative', 'submitted', 'that', 'it', 'was', 'related', 'to', 'loan', 'from', 'broker', ',', 'Rahul', '&', 'Co', '.', 'on', 'the', 'basis', 'of', 'his', 'submission', 'a', 'necessary', 'mark', 'is', 'put', 'by', 'us', 'on', 'that', 'photo', 'copy', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "# Get the first 100 token IDs\n",
        "token_ids = rob_train_tokenized['input_ids'][7][:100]\n",
        "\n",
        "# Convert token IDs to tokens\n",
        "tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
        "\n",
        "# Convert tokens to string\n",
        "tokens_string = tokenizer.convert_tokens_to_string(tokens)\n",
        "\n",
        "# Print the result\n",
        "# print(tokens_string)\n",
        "for token_id, token in zip(token_ids, tokens):\n",
        "    print(f\"Token ID: {token_id}, Token: {token}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "67KmiGi7O106",
        "outputId": "63f05122-a961-4ca1-ed72-c7dd32fa3866"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token ID: 0, Token: <s>\n",
            "Token ID: 4148, Token: On\n",
            "Token ID: 5, Token: the\n",
            "Token ID: 363, Token: night\n",
            "Token ID: 9, Token: of\n",
            "Token ID: 971, Token: 28\n",
            "Token ID: 494, Token: March\n",
            "Token ID: 6, Token: ,\n",
            "Token ID: 23342, Token: 1959\n",
            "Token ID: 6, Token: ,\n",
            "Token ID: 28606, Token: Krish\n",
            "Token ID: 8697, Token: nam\n",
            "Token ID: 710, Token: ur\n",
            "Token ID: 212, Token: th\n",
            "Token ID: 118, Token: i\n",
            "Token ID: 17347, Token: Rao\n",
            "Token ID: 6, Token: ,\n",
            "Token ID: 309, Token: according\n",
            "Token ID: 7, Token: to\n",
            "Token ID: 39, Token: his\n",
            "Token ID: 527, Token: story\n",
            "Token ID: 6, Token: ,\n",
            "Token ID: 3203, Token: walked\n",
            "Token ID: 88, Token: into\n",
            "Token ID: 5, Token: the\n",
            "Token ID: 929, Token: room\n",
            "Token ID: 9, Token: of\n",
            "Token ID: 5, Token: the\n",
            "Token ID: 31390, Token: petitioner\n",
            "Token ID: 6, Token: ,\n",
            "Token ID: 851, Token: gave\n",
            "Token ID: 5, Token: the\n",
            "Token ID: 25677, Token: bribe\n",
            "Token ID: 8, Token: and\n",
            "Token ID: 156, Token: made\n",
            "Token ID: 5, Token: the\n",
            "Token ID: 1198, Token: pre\n",
            "Token ID: 12, Token: -\n",
            "Token ID: 6166, Token: arr\n",
            "Token ID: 17770, Token: anged\n",
            "Token ID: 6029, Token: signal\n",
            "Token ID: 61, Token: which\n",
            "Token ID: 1146, Token: brought\n",
            "Token ID: 5, Token: the\n",
            "Token ID: 6267, Token: Assistant\n",
            "Token ID: 10313, Token: Superintendent\n",
            "Token ID: 9, Token: of\n",
            "Token ID: 522, Token: Police\n",
            "Token ID: 8, Token: and\n",
            "Token ID: 39, Token: his\n",
            "Token ID: 26470, Token: companions\n",
            "Token ID: 7, Token: to\n",
            "Token ID: 5, Token: the\n",
            "Token ID: 929, Token: room\n",
            "Token ID: 4, Token: .\n",
            "Token ID: 2, Token: </s>\n",
            "Token ID: 1, Token: <pad>\n",
            "Token ID: 1, Token: <pad>\n",
            "Token ID: 1, Token: <pad>\n",
            "Token ID: 1, Token: <pad>\n",
            "Token ID: 1, Token: <pad>\n",
            "Token ID: 1, Token: <pad>\n",
            "Token ID: 1, Token: <pad>\n",
            "Token ID: 1, Token: <pad>\n",
            "Token ID: 1, Token: <pad>\n",
            "Token ID: 1, Token: <pad>\n",
            "Token ID: 1, Token: <pad>\n",
            "Token ID: 1, Token: <pad>\n",
            "Token ID: 1, Token: <pad>\n",
            "Token ID: 1, Token: <pad>\n",
            "Token ID: 1, Token: <pad>\n",
            "Token ID: 1, Token: <pad>\n",
            "Token ID: 1, Token: <pad>\n",
            "Token ID: 1, Token: <pad>\n",
            "Token ID: 1, Token: <pad>\n",
            "Token ID: 1, Token: <pad>\n",
            "Token ID: 1, Token: <pad>\n",
            "Token ID: 1, Token: <pad>\n",
            "Token ID: 1, Token: <pad>\n",
            "Token ID: 1, Token: <pad>\n",
            "Token ID: 1, Token: <pad>\n",
            "Token ID: 1, Token: <pad>\n",
            "Token ID: 1, Token: <pad>\n",
            "Token ID: 1, Token: <pad>\n",
            "Token ID: 1, Token: <pad>\n",
            "Token ID: 1, Token: <pad>\n",
            "Token ID: 1, Token: <pad>\n",
            "Token ID: 1, Token: <pad>\n",
            "Token ID: 1, Token: <pad>\n",
            "Token ID: 1, Token: <pad>\n",
            "Token ID: 1, Token: <pad>\n",
            "Token ID: 1, Token: <pad>\n",
            "Token ID: 1, Token: <pad>\n",
            "Token ID: 1, Token: <pad>\n",
            "Token ID: 1, Token: <pad>\n",
            "Token ID: 1, Token: <pad>\n",
            "Token ID: 1, Token: <pad>\n",
            "Token ID: 1, Token: <pad>\n",
            "Token ID: 1, Token: <pad>\n",
            "Token ID: 1, Token: <pad>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "[AutoTokenizer.from_pretrained('roberta-base').convert_ids_to_tokens(tokenized) for tokenized in train_tokenized['input_ids'][0][:100]]\n",
        "# rob_train_tokenized[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7kwfsTFLwR0",
        "outputId": "075f0d7b-530f-49b5-c77a-5e543ed61a7e"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['like',\n",
              " '\"',\n",
              " \"'\",\n",
              " 'if',\n",
              " 'land',\n",
              " 'AR',\n",
              " 'ither',\n",
              " 'opportunities',\n",
              " 'compared',\n",
              " 'charges',\n",
              " 'contest',\n",
              " 'version',\n",
              " 'pressure',\n",
              " 'debt',\n",
              " 'Wh',\n",
              " 'white',\n",
              " 'human',\n",
              " 'm',\n",
              " 'now',\n",
              " 'no',\n",
              " 'figure',\n",
              " 'no',\n",
              " 'buildings',\n",
              " 'no',\n",
              " 'mostly',\n",
              " 'ana',\n",
              " 'petroleum',\n",
              " 'ter',\n",
              " 'white',\n",
              " 'headed',\n",
              " 'Filipino',\n",
              " 'self',\n",
              " 'familiar',\n",
              " 'fill',\n",
              " 'white',\n",
              " 'groups',\n",
              " 'We',\n",
              " 'settle',\n",
              " 'teammates',\n",
              " 'terms',\n",
              " 'crews',\n",
              " 'ard',\n",
              " 'take',\n",
              " 'm',\n",
              " 'perfect',\n",
              " 'white',\n",
              " 'Mirror',\n",
              " 'walking',\n",
              " '1',\n",
              " 'New',\n",
              " 'standard',\n",
              " 'banks',\n",
              " 'no',\n",
              " 'Credit',\n",
              " 'amassed',\n",
              " 'Affairs',\n",
              " 'divorce',\n",
              " 'ap',\n",
              " 'similar',\n",
              " 'break',\n",
              " 'green',\n",
              " 'If',\n",
              " 'esh',\n",
              " 'In',\n",
              " 'rusher',\n",
              " 'no',\n",
              " 'fairness',\n",
              " 'admiration',\n",
              " '-',\n",
              " 'brings',\n",
              " 'm',\n",
              " 'ana',\n",
              " 'charges',\n",
              " 'veteran',\n",
              " 'white',\n",
              " 'nine',\n",
              " 'famously',\n",
              " 'We',\n",
              " 'episode',\n",
              " 'failing',\n",
              " 'terms',\n",
              " 'mind',\n",
              " 'compared',\n",
              " '18',\n",
              " 'ana',\n",
              " 'ap',\n",
              " 'settle',\n",
              " 'teammates',\n",
              " 'm',\n",
              " 'a',\n",
              " '<s>',\n",
              " '<s>',\n",
              " '<s>',\n",
              " '<s>',\n",
              " '<s>',\n",
              " '<s>',\n",
              " '<s>',\n",
              " '<s>',\n",
              " '<s>',\n",
              " '<s>']"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_tokenized['input_ids'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "USyII4UEMobh",
        "outputId": "6f553b71-99a2-4ed7-fc64-bfc46b54de4b"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[101,\n",
              " 113,\n",
              " 128,\n",
              " 114,\n",
              " 1212,\n",
              " 2747,\n",
              " 15027,\n",
              " 1616,\n",
              " 1118,\n",
              " 1103,\n",
              " 3096,\n",
              " 1732,\n",
              " 1164,\n",
              " 1126,\n",
              " 3990,\n",
              " 1104,\n",
              " 19003,\n",
              " 119,\n",
              " 122,\n",
              " 117,\n",
              " 1955,\n",
              " 117,\n",
              " 3413,\n",
              " 117,\n",
              " 2260,\n",
              " 1113,\n",
              " 14304,\n",
              " 1334,\n",
              " 1104,\n",
              " 3475,\n",
              " 19890,\n",
              " 1403,\n",
              " 2950,\n",
              " 3300,\n",
              " 1104,\n",
              " 1134,\n",
              " 170,\n",
              " 6307,\n",
              " 5633,\n",
              " 1110,\n",
              " 5452,\n",
              " 1120,\n",
              " 185,\n",
              " 119,\n",
              " 1969,\n",
              " 1104,\n",
              " 15187,\n",
              " 3051,\n",
              " 112,\n",
              " 188,\n",
              " 2526,\n",
              " 1520,\n",
              " 117,\n",
              " 3560,\n",
              " 21812,\n",
              " 4702,\n",
              " 7402,\n",
              " 1115,\n",
              " 1122,\n",
              " 1108,\n",
              " 2272,\n",
              " 1106,\n",
              " 4891,\n",
              " 1121,\n",
              " 24535,\n",
              " 117,\n",
              " 16890,\n",
              " 24287,\n",
              " 111,\n",
              " 3291,\n",
              " 119,\n",
              " 1113,\n",
              " 1103,\n",
              " 3142,\n",
              " 1104,\n",
              " 1117,\n",
              " 13455,\n",
              " 170,\n",
              " 3238,\n",
              " 4551,\n",
              " 1110,\n",
              " 1508,\n",
              " 1118,\n",
              " 1366,\n",
              " 1113,\n",
              " 1115,\n",
              " 6307,\n",
              " 5633,\n",
              " 119,\n",
              " 102,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### affect test"
      ],
      "metadata": {
        "id": "RT6xXsAZTkK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_tokens_labels = affect(train_data, rob_train_tokenized, train_text_data, rob_train_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "td9OX7wdKIvy",
        "outputId": "705ec8fa-bafb-4efa-866e-dd908c231886"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u0013 slice(112, 113, None)\n",
            "not so special entity\n",
            "here?! 1068 1070 Union of India                           v. Jai Narayan Singh,  1995 Supp. (4) 672 ['.', '</s>'] \t slice(46, 48, None) \t slice(420, 502, None) slice(420, 502, None)\n",
            "here?! 1068 1070 State of H.P. v. Suresh Kumar Verma ['.', '</s>'] \t slice(46, 48, None) \t slice(707, 742, None) slice(707, 742, None)\n",
            "here?! 5824 5825 --'Paupuk Kannu Anni v. Thoppayya Mudaliar', (J) :   [\"'\", 'P', 'au', 'p', 'uk', 'K', 'ann', 'u', 'Ann', 'i', 'v', '.', 'Th', 'opp', 'ay', 'ya', 'Mud', 'ali', 'ar', \"',\", '(', 'J', ')', ':', '', 'Clause'] \t slice(14, 40, None) \t slice(38, 90, None) slice(39, 89, None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_text_data[1068][420:502]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vdvd6-OnmsG9",
        "outputId": "aaf6d23b-4afc-4794-8243-d2816e719b1e"
      },
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Union of India                           v. Jai Narayan Singh,  1995 Supp. (4) 672'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for indice in range(len(train_text_data)):\n",
        "  if rob_train_tokenized['overflow_to_sample_mapping'].count(indice) > 1:\n",
        "    print(indice)\n",
        "rob_train_tokenized['overflow_to_sample_mapping'].count(1068), rob_train_tokenized['overflow_to_sample_mapping'].index(1068)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzBxmqAgniWu",
        "outputId": "8a38371b-eaac-43a7-928b-0bb240310a3c"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1068\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 1068)"
            ]
          },
          "metadata": {},
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_pos = slice(420, 502)\n",
        "tkn_indice = 1068\n",
        "i=0\n",
        "clean_pos.start > rob_train_tokenized[\"offset_mapping\"][tkn_indice+i+1][2][0], clean_pos.stop < max(rob_train_tokenized[\"offset_mapping\"][tkn_indice+i+1])[1]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9ZkBkt5oZbm",
        "outputId": "e25846aa-6241-4a6e-b76d-2138075b888f"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(False, True)"
            ]
          },
          "metadata": {},
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_text_data), len(rob_train_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVjkdmZvnQTf",
        "outputId": "ac1884cc-245a-4413-8d93-66fc78899010"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9435, 9436)"
            ]
          },
          "metadata": {},
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "# Get the first 100 token IDs\n",
        "my_token_ids = rob_train_tokenized['input_ids'][1068]#[:100]\n",
        "\n",
        "# Convert token IDs to tokens\n",
        "my_tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
        "\n",
        "# Convert tokens to string\n",
        "my_tokens_string = tokenizer.convert_tokens_to_string(tokens)\n",
        "\n",
        "# Print the result\n",
        "# print(tokens_string)\n",
        "# for token_id, token in zip(my_token_ids, my_tokens):\n",
        "#     print(f\"Token ID: {token_id}, Token: {token}\")\n",
        "my_tokens_string#[420:502]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "N4xRawlXmEI6",
        "outputId": "5d0480f4-004b-4172-c469-13539a686d03"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<s>On the night of 28 March, 1959, Krishnamurthi Rao, according to his story, walked into the room of the petitioner, gave the bribe and made the pre-arranged signal which brought the Assistant Superintendent of Police and his companions to the room.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[4]['data']['text'], train_text_data[3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EdxikHQUVeM",
        "outputId": "21eb32d1-8d42-4755-ff13-d5313c569db4"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('The pillion rider T.V. Satyanarayana Murthy also sustained injuries.',\n",
              " 'The pillion rider T.V. Satyanarayana Murthy also sustained injuries.')"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_tokenized[\"offset_mapping\"][0]\n",
        "kk = [['O' for mapper in train_tokenized[\"offset_mapping\"][i]] for i in range(len(train_tokenized[\"offset_mapping\"]))]"
      ],
      "metadata": {
        "id": "8rdGOnxRJ6Uq"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_ne_dist(the_tokens_labels, ne_distribution ):\n",
        "  for labels in the_tokens_labels:\n",
        "    for idx in range(len(labels)):\n",
        "      if labels[idx] in ne_distribution.keys():\n",
        "        ne_distribution[labels[idx]] +=1\n",
        "      else:\n",
        "        ne_distribution[labels[idx]] = 1\n",
        "  return ne_distribution\n",
        "\n",
        "ne_distribution = {}\n",
        "ne_distribution = update_ne_dist(tokens_labels, ne_distribution)\n",
        "ne_distribution = update_ne_dist(dev_tokens_labels, ne_distribution)\n",
        "\n",
        "labels_list = list(ne_distribution.keys())\n"
      ],
      "metadata": {
        "id": "vHD7H7WxBBTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_int_tokens_lab(the_tokens_labels):\n",
        "  import copy\n",
        "  the_int_tokens_labels = copy.deepcopy(the_tokens_labels)\n",
        "\n",
        "  for i in range(len(the_tokens_labels)):\n",
        "    for j in range(len(the_tokens_labels[i])):\n",
        "      the_int_tokens_labels[i][j] = int_labels[the_tokens_labels[i][j]]\n",
        "\n",
        "int_tokens_labels = get_int_tokens_lab(tokens_labels)\n",
        "int_tokens_labels = get_int_tokens_lab(dev_tokens_labels)"
      ],
      "metadata": {
        "id": "qGZmzyJyhCv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#appearently I used the with_snnot_id in data gen"
      ],
      "metadata": {
        "id": "QLj2UosjBBZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dtst_gen(the_json_data, the_int_tokens_labels, the_tokenized):\n",
        "  for id, content in enumerate(the_json_data): #content is basically the dct with its annotations\n",
        "    if content['annotations'] == [{'result': []}]:  #following the same logic and only use the annotated data\n",
        "      yield {'labels' : the_int_tokens_labels[id], 'input_ids': the_tokenized['input_ids'][id] , 'attention_mask': the_tokenized['attention_mask'][id]}\n",
        "\n",
        "# from datasets import Dataset\n",
        "# no_ne_gen = lambda: dtst_gen(json_data, int_tokens_labels, tokenized) #we need to define it using lambda otherwise we might immediately call the generator\n",
        "# with_ne_train_dt = Dataset.from_generator(no_ne_gen)\n",
        "\n",
        "# dev_gen = lambda: dtst_gen(dev_data, dev_int_tokens_labels, dev_tokenized)\n",
        "# dev_dt = Dataset.from_generator(dev_gen)\n"
      ],
      "metadata": {
        "id": "gzsZmEjpBBeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Pre_processing:\n",
        "    def __init__(self):\n",
        "\n",
        "    def drop(self, data, drop_strategies):\n",
        "        return self.data\n",
        "\n",
        "    def fillna(self, ntrain, fill_strategies):\n",
        "        def fill(column, fill_with):\n",
        "\n",
        "    def feature_engineering(self):\n",
        "\n",
        "    def label_encoder(self, columns):\n",
        "\n",
        "    def get_dummies(self, columns):\n",
        "\n",
        "    def norm_data(self, columns):\n"
      ],
      "metadata": {
        "id": "K_NkVYWp86mY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Pre_processing:\n",
        "    \"\"\"\n",
        "    This class prepares the data berfore applying ML\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "\n",
        "        print()\n",
        "        print('pre-processing object is created')\n",
        "        print()\n",
        "\n",
        "    def drop(self, data, drop_strategies):\n",
        "        \"\"\"\n",
        "        This function is used to drop a column or row from the dataset.\n",
        "        ...\n",
        "        Attributes\n",
        "        ----------\n",
        "        data : Pandas DataFrame\n",
        "            The data you want to drop data from.\n",
        "        drop_strategies : A list of tuples, each tuple has the data to drop,\n",
        "        and the axis(0 or 1)\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        A new dataset after dropping the unwanted data.\n",
        "        \"\"\"\n",
        "\n",
        "        self.data=data\n",
        "\n",
        "        for columns, ax in drop_strategies:\n",
        "            if len(columns)==1:\n",
        "                self.data=self.data.drop(labels=column, axis=ax)\n",
        "            else:\n",
        "                for column in columns:\n",
        "                    self.data=self.data.drop(labels=column, axis=ax)\n",
        "        return self.data\n",
        "\n",
        "    def fillna(self, ntrain, fill_strategies):\n",
        "        \"\"\"\n",
        "        This function fills NA/NaN values in a specific column using a specified method(zero,mean,...)\n",
        "        ...\n",
        "        Attributes\n",
        "        ----------\n",
        "        data : Pandas DataFrame\n",
        "            The data you want to impute its missing values\n",
        "        fill_strategies : A dictionary, its keys represent the columns,\n",
        "        and the values represent the value to use to fill the Nulls.\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        A new dataset without null values.\n",
        "        \"\"\"\n",
        "        def fill(column, fill_with):\n",
        "\n",
        "                if str(fill_with).lower() in ['zero', 0]:\n",
        "                    self.data[column].fillna(0, inplace=True)\n",
        "                elif str(fill_with).lower()=='mode':\n",
        "                    self.data[column].fillna(self.data[column].mode()[0], inplace=True)\n",
        "                elif str(fill_with).lower()=='mean':\n",
        "                    self.data[column].fillna(self.data[column].mean(), inplace=True)\n",
        "                elif str(fill_with).lower()=='median':\n",
        "                    self.data[column].fillna(self.data[column].median(), inplace=True)\n",
        "                else:\n",
        "                    self.data[column].fillna(fill_with, inplace=True)\n",
        "\n",
        "                return self.data\n",
        "\n",
        "        #LotFrontage: Linear feet of street connected to property\n",
        "        self.data['LotFrontage'] = self.data.groupby('Neighborhood')['LotFrontage'].apply(lambda x: x.fillna(x.median())).values\n",
        "\n",
        "        # Meaning that NO Masonry veneer\n",
        "        self.data['MSZoning'] = self.data['MSZoning'].transform(lambda x: x.fillna(x.mode().values[0]))\n",
        "\n",
        "        #imputing columns according to its strategy\n",
        "        for columns, strategy in fill_strategies:\n",
        "            if len(columns)==1:\n",
        "                fill(columns[0], strategy)\n",
        "            else:\n",
        "                for column in columns:\n",
        "                    fill(column, strategy)\n",
        "\n",
        "        return self.data\n",
        "\n",
        "    def feature_engineering(self):\n",
        "        \"\"\"\n",
        "        This function is used to apply some feature engineering on the data.\n",
        "        ...\n",
        "        Attributes\n",
        "        ----------\n",
        "        data : Pandas DataFrame\n",
        "            The data you want to apply feature engineering on.\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        A new dataset with new columns and some additions.\n",
        "        \"\"\"\n",
        "        # creating new columns\n",
        "        self.data['TotalSF'] = self.data['TotalBsmtSF'] + self.data['1stFlrSF'] + self.data['2ndFlrSF']\n",
        "\n",
        "        # Convert some columns from numeric to string\n",
        "        self.data[['YrSold','MSSubClass','MoSold','OverallCond']] = self.data[['YrSold','MSSubClass','MoSold','OverallCond']].astype(str)\n",
        "\n",
        "        # Convert some columns from numeric to int\n",
        "        self.data[['BsmtHalfBath','BsmtFinSF1', 'BsmtFinSF2','BsmtFullBath','BsmtUnfSF','GarageCars','GarageArea']]\\\n",
        "        =self.data[['BsmtHalfBath','BsmtFinSF1', 'BsmtFinSF2','BsmtFullBath','BsmtUnfSF','GarageCars','GarageArea']].astype(int)\n",
        "\n",
        "        return self.data\n",
        "\n",
        "    def label_encoder(self, columns):\n",
        "        \"\"\"\n",
        "        This function is used to encode the data to categorical values to benefit from increasing or\n",
        "        decreasing to build the model\n",
        "        ...\n",
        "        Attributes\n",
        "        ----------\n",
        "        data : Pandas DataFrame\n",
        "            The data you want to encode.\n",
        "        columns : columns to convert.\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        A dataset without categorical data.\n",
        "        \"\"\"\n",
        "\n",
        "        # Convert all categorical collumns to numeric values\n",
        "        lbl = LabelEncoder()\n",
        "\n",
        "        self.data[columns] = self.data[columns].apply(lambda x:lbl.fit_transform(x.astype(str)).astype(int))\n",
        "\n",
        "        return self.data\n",
        "\n",
        "    def get_dummies(self, columns):\n",
        "        \"\"\"\n",
        "        This function is used to convert the data to dummies values.\n",
        "        ...\n",
        "        Attributes\n",
        "        ----------\n",
        "        data : Pandas DataFrame\n",
        "            The data you want to convert.\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        A dataset with dummies.\n",
        "        \"\"\"\n",
        "\n",
        "        # convert our categorical columns to dummies\n",
        "        for col in columns:\n",
        "            dumm = pd.get_dummies(self.data[col], prefix = col, dtype=int)\n",
        "            self.data = pd.concat([self.data, dumm], axis=1)\n",
        "\n",
        "        self.data.drop(columns, axis=1, inplace=True)\n",
        "\n",
        "        return self.data\n",
        "\n",
        "    def norm_data(self, columns):\n",
        "        \"\"\"\n",
        "        This function is used to normalize the data.\n",
        "        ...\n",
        "        Attributes\n",
        "        ----------\n",
        "        data : Pandas DataFrame\n",
        "            The data you want to normalize.\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        A new normalized dataset.\n",
        "        \"\"\"\n",
        "\n",
        "        # Normalize our numeric data\n",
        "        self.data[columns] = self.data[columns].apply(lambda x:np.log1p(x)) #Normalize the data with Logarithms\n",
        "\n",
        "        return self.data"
      ],
      "metadata": {
        "id": "OYR8oTTr86mZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataPrep():\n",
        "\n",
        "  def clean_text_data(text_data):\n",
        "    import re\n",
        "    replacements = [\n",
        "        (r'\\.{2,}', ' '),\n",
        "        (r'\\={2,}', ' '),\n",
        "        (r'\\-{2,}', ' '),\n",
        "        (r'\\xad', ' '),\n",
        "        (r'\\xa0', ' '),\n",
        "        (r'\\x80', ''),\n",
        "        (r'\\x9d', ''),\n",
        "        (r'\\x13', ' '),\n",
        "        ]\n",
        "\n",
        "    for old, new in replacements:\n",
        "      text_data = re.sub(old, new, text_data)\n",
        "    return text_data\n",
        "\n",
        "  def create_text_data(raw_json_data):\n",
        "    the_text_data = []\n",
        "    for line in raw_json_data:\n",
        "      if line['annotations'] != ''\n",
        "\n",
        "      the_text_data.append(clean_text_data(line['data']['text']))\n",
        "    return the_text_data\n",
        "\n",
        "\n",
        "\n",
        "  dev_text_data = create_text_data(dev_data)\n",
        "  train_text_data =\n",
        "  plain_text_data = []\n",
        "  # for line in json_data:\n",
        "  for idx in with_annot_indices:\n",
        "      text_data.append(clean_text_data(json_data[idx]['data']['text']))\n",
        "      plain_text_data.append(json_data[idx]['data']['text'])\n",
        "\n",
        "  def get_new_position(old_text, new_text, pos):\n",
        "    #a function that'll allow to find the position of a special entity in the new cleaned text using the position in the raw text given in annotations\n",
        "\n",
        "    if new_text[pos] == old_text[pos]: #pos did not change\n",
        "      return pos\n",
        "\n",
        "    else: # we actually need to update the pos\n",
        "\n",
        "      special_entity = old_text[pos]\n",
        "      if clean_text_data(special_entity).split() == []:\n",
        "        return 'not so special entity'\n",
        "\n",
        "      if special_entity[0:2] == '--':\n",
        "        special_entity = special_entity[2:] #cases where the NE starts with -, preceeded by -- in data['text'] and omitted by the cleaninh function\n",
        "        pos = slice(pos.start + 2, pos.stop)\n",
        "\n",
        "      if special_entity[0:1] == '-' and special_entity[1:2] != '-':\n",
        "        special_entity = special_entity[1:] #cases where the NE starts with -, preceeded by -- in data['text'] and omitted by the cleaninh function\n",
        "        pos = slice(pos.start + 1, pos.stop)\n",
        "\n",
        "      in_old_text = [m.start() for m in re.finditer(re.escape(special_entity), old_text)]\n",
        "      in_new_text = [m.start() for m in re.finditer(re.escape(clean_text_data(special_entity)), new_text)]\n",
        "\n",
        "      if len(in_new_text) != len(in_old_text) and special_entity != '.':  #written differently #with whitespaces, or special characters\n",
        "        # this block is for test, and shouldn't be achieved\n",
        "        print(indice, \"\\n not same len\")\n",
        "        print('\\t special_entity: ', special_entity, 'at pos', pos)\n",
        "        print('in_old_text: ', in_old_text, '\\t in_new_text: ',in_new_text)\n",
        "\n",
        "      else:\n",
        "\n",
        "        all_occ = dict(zip(in_old_text,in_new_text))\n",
        "\n",
        "        start = all_occ[pos.start]\n",
        "\n",
        "        end = start+len(clean_text_data(old_text[pos]))#-1\n",
        "\n",
        "        # check\n",
        "        if clean_text_data(old_text[pos]) != new_text[slice(start, end)]:\n",
        "          print('not similar', old_text[pos], '\\t', new_text[slice(start, end)])\n",
        "\n",
        "\n",
        "        return slice(start,end)\n"
      ],
      "metadata": {
        "id": "mgn_V--lykPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_model_tokenize(model_name, text_data, dev_text_data ):\n",
        "  model = model_name\n",
        "  from transformers import AutoTokenizer\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "\n",
        "  tokenized = tokenizer(text_data,  is_split_into_words = False, return_offsets_mapping = True, add_special_tokens = True, truncation = True, padding = 'max_length', max_length=512, stride=128, return_overflowing_tokens=True,\n",
        "  )  #when text is not splitted, the positions in str are accurate\n",
        "\n",
        "  tokens = [tokenizer.convert_ids_to_tokens(tokenizedx) for tokenizedx in tokenized.input_ids]\n",
        "\n",
        "  dev_tokenized = tokenizer(dev_text_data,  is_split_into_words = False,  return_offsets_mapping = True, add_special_tokens = True,  padding = 'max_length', truncation = True, max_length=512, stride=128, return_overflowing_tokens=True, )  #when text is not splitted, the positions are accurate\n",
        "\n",
        "  dev_tokens = [tokenizer.convert_ids_to_tokens(tokenizedx) for tokenizedx in dev_tokenized.input_ids]\n",
        "\n",
        "  return tokenized, dev_tokenized, tokens, dev_tokens\n",
        "\n",
        "tokenized, dev_tokenized, tokens, dev_tokens = set_model_tokenize(model_name, text_data, dev_text_data )"
      ],
      "metadata": {
        "id": "VxzAYtZ7yrf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Pre_processing:\n",
        "    def __init__(self):\n",
        "\n",
        "    def drop(self, data, drop_strategies):\n",
        "        return self.data\n",
        "\n",
        "    def fillna(self, ntrain, fill_strategies):\n",
        "        def fill(column, fill_with):\n",
        "\n",
        "    def feature_engineering(self):\n",
        "\n",
        "    def label_encoder(self, columns):\n",
        "\n",
        "    def get_dummies(self, columns):\n",
        "\n",
        "    def norm_data(self, columns):\n"
      ],
      "metadata": {
        "id": "Kwh615Oe3_It"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Pre_processing:\n",
        "    \"\"\"\n",
        "    This class prepares the data berfore applying ML\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "\n",
        "        print()\n",
        "        print('pre-processing object is created')\n",
        "        print()\n",
        "\n",
        "    def drop(self, data, drop_strategies):\n",
        "        \"\"\"\n",
        "        This function is used to drop a column or row from the dataset.\n",
        "        ...\n",
        "        Attributes\n",
        "        ----------\n",
        "        data : Pandas DataFrame\n",
        "            The data you want to drop data from.\n",
        "        drop_strategies : A list of tuples, each tuple has the data to drop,\n",
        "        and the axis(0 or 1)\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        A new dataset after dropping the unwanted data.\n",
        "        \"\"\"\n",
        "\n",
        "        self.data=data\n",
        "\n",
        "        for columns, ax in drop_strategies:\n",
        "            if len(columns)==1:\n",
        "                self.data=self.data.drop(labels=column, axis=ax)\n",
        "            else:\n",
        "                for column in columns:\n",
        "                    self.data=self.data.drop(labels=column, axis=ax)\n",
        "        return self.data\n",
        "\n",
        "    def fillna(self, ntrain, fill_strategies):\n",
        "        \"\"\"\n",
        "        This function fills NA/NaN values in a specific column using a specified method(zero,mean,...)\n",
        "        ...\n",
        "        Attributes\n",
        "        ----------\n",
        "        data : Pandas DataFrame\n",
        "            The data you want to impute its missing values\n",
        "        fill_strategies : A dictionary, its keys represent the columns,\n",
        "        and the values represent the value to use to fill the Nulls.\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        A new dataset without null values.\n",
        "        \"\"\"\n",
        "        def fill(column, fill_with):\n",
        "\n",
        "                if str(fill_with).lower() in ['zero', 0]:\n",
        "                    self.data[column].fillna(0, inplace=True)\n",
        "                elif str(fill_with).lower()=='mode':\n",
        "                    self.data[column].fillna(self.data[column].mode()[0], inplace=True)\n",
        "                elif str(fill_with).lower()=='mean':\n",
        "                    self.data[column].fillna(self.data[column].mean(), inplace=True)\n",
        "                elif str(fill_with).lower()=='median':\n",
        "                    self.data[column].fillna(self.data[column].median(), inplace=True)\n",
        "                else:\n",
        "                    self.data[column].fillna(fill_with, inplace=True)\n",
        "\n",
        "                return self.data\n",
        "\n",
        "        #LotFrontage: Linear feet of street connected to property\n",
        "        self.data['LotFrontage'] = self.data.groupby('Neighborhood')['LotFrontage'].apply(lambda x: x.fillna(x.median())).values\n",
        "\n",
        "        # Meaning that NO Masonry veneer\n",
        "        self.data['MSZoning'] = self.data['MSZoning'].transform(lambda x: x.fillna(x.mode().values[0]))\n",
        "\n",
        "        #imputing columns according to its strategy\n",
        "        for columns, strategy in fill_strategies:\n",
        "            if len(columns)==1:\n",
        "                fill(columns[0], strategy)\n",
        "            else:\n",
        "                for column in columns:\n",
        "                    fill(column, strategy)\n",
        "\n",
        "        return self.data\n",
        "\n",
        "    def feature_engineering(self):\n",
        "        \"\"\"\n",
        "        This function is used to apply some feature engineering on the data.\n",
        "        ...\n",
        "        Attributes\n",
        "        ----------\n",
        "        data : Pandas DataFrame\n",
        "            The data you want to apply feature engineering on.\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        A new dataset with new columns and some additions.\n",
        "        \"\"\"\n",
        "        # creating new columns\n",
        "        self.data['TotalSF'] = self.data['TotalBsmtSF'] + self.data['1stFlrSF'] + self.data['2ndFlrSF']\n",
        "\n",
        "        # Convert some columns from numeric to string\n",
        "        self.data[['YrSold','MSSubClass','MoSold','OverallCond']] = self.data[['YrSold','MSSubClass','MoSold','OverallCond']].astype(str)\n",
        "\n",
        "        # Convert some columns from numeric to int\n",
        "        self.data[['BsmtHalfBath','BsmtFinSF1', 'BsmtFinSF2','BsmtFullBath','BsmtUnfSF','GarageCars','GarageArea']]\\\n",
        "        =self.data[['BsmtHalfBath','BsmtFinSF1', 'BsmtFinSF2','BsmtFullBath','BsmtUnfSF','GarageCars','GarageArea']].astype(int)\n",
        "\n",
        "        return self.data\n",
        "\n",
        "    def label_encoder(self, columns):\n",
        "        \"\"\"\n",
        "        This function is used to encode the data to categorical values to benefit from increasing or\n",
        "        decreasing to build the model\n",
        "        ...\n",
        "        Attributes\n",
        "        ----------\n",
        "        data : Pandas DataFrame\n",
        "            The data you want to encode.\n",
        "        columns : columns to convert.\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        A dataset without categorical data.\n",
        "        \"\"\"\n",
        "\n",
        "        # Convert all categorical collumns to numeric values\n",
        "        lbl = LabelEncoder()\n",
        "\n",
        "        self.data[columns] = self.data[columns].apply(lambda x:lbl.fit_transform(x.astype(str)).astype(int))\n",
        "\n",
        "        return self.data\n",
        "\n",
        "    def get_dummies(self, columns):\n",
        "        \"\"\"\n",
        "        This function is used to convert the data to dummies values.\n",
        "        ...\n",
        "        Attributes\n",
        "        ----------\n",
        "        data : Pandas DataFrame\n",
        "            The data you want to convert.\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        A dataset with dummies.\n",
        "        \"\"\"\n",
        "\n",
        "        # convert our categorical columns to dummies\n",
        "        for col in columns:\n",
        "            dumm = pd.get_dummies(self.data[col], prefix = col, dtype=int)\n",
        "            self.data = pd.concat([self.data, dumm], axis=1)\n",
        "\n",
        "        self.data.drop(columns, axis=1, inplace=True)\n",
        "\n",
        "        return self.data\n",
        "\n",
        "    def norm_data(self, columns):\n",
        "        \"\"\"\n",
        "        This function is used to normalize the data.\n",
        "        ...\n",
        "        Attributes\n",
        "        ----------\n",
        "        data : Pandas DataFrame\n",
        "            The data you want to normalize.\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        A new normalized dataset.\n",
        "        \"\"\"\n",
        "\n",
        "        # Normalize our numeric data\n",
        "        self.data[columns] = self.data[columns].apply(lambda x:np.log1p(x)) #Normalize the data with Logarithms\n",
        "\n",
        "        return self.data"
      ],
      "metadata": {
        "id": "0e69cTNgy9q2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}